{"cells":[{"cell_type":"markdown","metadata":{"id":"aSOSCYfKErjE"},"source":["# ECSE 415 Final Project: Classification (SIFT) (35 Points)\n","\n","Group 10 \n","\n","April 2022"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2828,"status":"ok","timestamp":1648917066713,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"q51P8JK2EkrN","outputId":"5086d991-b25b-44e3-8bd3-e2aed882bbfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","'''\n","Common google drive for project files and dataset. Mount with this drive.\n","email: ecse415project2022@gmail.com\n","password: mcgillecse415\n","'''\n","\n","path = \"/content/drive/MyDrive/ecse415-project/dataset\"\n","# path = \"/content/drive/MyDrive/ecse415-project/dataset_subset\"  # SUBSET PATH TO CHANGE FOR ACTUAL TRAINING\n","# test_path = \"/content/drive/MyDrive/ecse415-project/dataset_tyler_test\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13064,"status":"ok","timestamp":1648917079765,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"nbNDGtpOK-Bn","outputId":"09c7d531-aece-4636-ad52-05425b1dbe4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python==4.4.0.44 in /usr/local/lib/python3.7/dist-packages (4.4.0.44)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.4.0.44) (1.21.5)\n","Requirement already satisfied: opencv-contrib-python==4.4.0.44 in /usr/local/lib/python3.7/dist-packages (4.4.0.44)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.4.0.44) (1.21.5)\n"]}],"source":["#The version of cv2 needed is 4.4. As mentioned in Tutorial 4, the following command must be run to install the appropriate cv2 library.\n","!pip install opencv-python==4.4.0.44\n","!pip install opencv-contrib-python==4.4.0.44"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPuKvMGyEmwl"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from skimage import io\n","import cv2\n","import os\n","import pickle\n","\n","from sklearn.svm import LinearSVC, SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"cDhtdEwKQc5H"},"source":["## Extract Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2Ky1AV1ErG4"},"outputs":[],"source":["def images_to_arr(dataset_path, resize_shape=(100, 100)):\n","\n","\n","  veh_path = os.path.join(dataset_path, \"vehicle\")\n","  non_veh_path = os.path.join(dataset_path, \"non_vehicle\")\n","\n","  veh_listdir = os.listdir(veh_path)\n","  non_veh_listdir = os.listdir(non_veh_path)\n","  \n","  veh_images = []\n","  non_veh_images = []\n","\n","  for f in veh_listdir:\n","    img = cv2.imread(os.path.join(veh_path, f))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    # img = cv2.resize(img, resize_shape)\n","    veh_images.append(img)\n","  \n","  for f in non_veh_listdir:\n","    img = cv2.imread(os.path.join(non_veh_path, f))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    # img = cv2.resize(img, resize_shape)\n","    non_veh_images.append(img)\n","\n","  # concat vehicles and non-vehicles\n","  images = np.concatenate([np.array(veh_images), np.array(non_veh_images)])\n","  labels = np.concatenate([np.full(len(veh_listdir), 1.), np.full(len(non_veh_listdir), 0.)])\n","\n","  return images, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4589093,"status":"ok","timestamp":1648356193310,"user":{"displayName":"ECSE 415","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01135113846004908202"},"user_tz":240},"id":"K2waQNfDIVMd","outputId":"cbe0da93-0e91-4b8c-aee0-8e9d0bf80c3a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]}],"source":["# DANGER: only run if the files don't already exist\n","images_0000, labels_0000 = images_to_arr(path + \"/0000\")\n","np.save(os.path.join(path, 'images_0000'), images_0000)\n","np.save(os.path.join(path, 'labels_0000'), labels_0000)\n","\n","images_0001, labels_0001 = images_to_arr(path + \"/0001\")\n","np.save(os.path.join(path, 'images_0001'), images_0001)\n","np.save(os.path.join(path, 'labels_0001'), labels_0001)\n","\n","images_0002, labels_0002 = images_to_arr(path + \"/0002\")\n","np.save(os.path.join(path, 'images_0002'), images_0002)\n","np.save(os.path.join(path, 'labels_0002'), labels_0002)\n","\n","images_0003, labels_0003 = images_to_arr(path + \"/0003\")\n","np.save(os.path.join(path, 'images_0003'), images_0003)\n","np.save(os.path.join(path, 'labels_0003'), labels_0003)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6q75UGvAcu0t"},"outputs":[],"source":["images_0000 = np.load(os.path.join(path, 'images_0000.npy'), allow_pickle=True)\n","images_0001 = np.load(os.path.join(path, 'images_0001.npy'), allow_pickle=True)\n","images_0002 = np.load(os.path.join(path, 'images_0002.npy'), allow_pickle=True)\n","images_0003 = np.load(os.path.join(path, 'images_0003.npy'), allow_pickle=True)\n","labels_0000 = np.load(os.path.join(path, 'labels_0000.npy'), allow_pickle=True)\n","labels_0001 = np.load(os.path.join(path, 'labels_0001.npy'), allow_pickle=True)\n","labels_0002 = np.load(os.path.join(path, 'labels_0002.npy'), allow_pickle=True)\n","labels_0003 = np.load(os.path.join(path, 'labels_0003.npy'), allow_pickle=True)\n","extra_images_0000 = np.load(os.path.join(path, 'extra_images_0000.npy'), allow_pickle=True)\n","extra_images_0001 = np.load(os.path.join(path, 'extra_images_0001.npy'), allow_pickle=True)\n","extra_images_0002 = np.load(os.path.join(path, 'extra_images_0002.npy'), allow_pickle=True)\n","extra_images_0003 = np.load(os.path.join(path, 'extra_images_0003.npy'), allow_pickle=True)\n","extra_labels_0000 = np.load(os.path.join(path, 'extra_labels_0000.npy'), allow_pickle=True)\n","extra_labels_0001 = np.load(os.path.join(path, 'extra_labels_0001.npy'), allow_pickle=True)\n","extra_labels_0002 = np.load(os.path.join(path, 'extra_labels_0002.npy'), allow_pickle=True)\n","extra_labels_0003 = np.load(os.path.join(path, 'extra_labels_0003.npy'), allow_pickle=True)"]},{"cell_type":"markdown","metadata":{"id":"uySQB-reQ8jE"},"source":["## Classifier\n"]},{"cell_type":"markdown","metadata":{"id":"AgtQ-Q4URXKK"},"source":["#### Description of Idea"]},{"cell_type":"markdown","metadata":{"id":"SrSCCWL3RLzY"},"source":["SVM Classifier Idea using SIFT:\n","\n","1. extract SIFT features for each training sample\n","\n","      -- optionally select a random number N of keypoints for each image\n","\n","2. compute K-means over all the N keypoints from each image (total of N*#images)\n","  \n","      -- optionally use mean-shift to do this to dynamically determine clusters\n","  \n","3. re-extract SIFT features for each training sample and create a histogram based on which clusters the features fall into. This will give us a vector of leangth K for each image with each value in the vector indicating the weight of that cluster in that image \n","\n","      -- this vector should be L1 normalized\n","\n","4. build an SVM classifier that takes in that K-vector for each image"]},{"cell_type":"markdown","metadata":{"id":"pz04GGSUQ-TD"},"source":["### 1 - SIFT Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVezj1qVOIOJ"},"outputs":[],"source":["NUM_DESC = 50\n","\n","def extract_all_descriptors(images, labels, num_desc):\n","  \"\"\"\n","  Given a list of images with their labels, this function returns 2 numpy arrays.\n","  The first array will be a list of all descriptors from each image. We do however\n","  limit each image to only provide ${num_desc} number of descriptors.\n","  \"\"\"\n","\n","  all_desc = []\n","  descriptors = np.empty((0, 128))\n","  for i, img in enumerate(images):\n","    # use gray image to get keypoints?\n","    # gray= cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n","    feature_extractor = cv2.SIFT_create()\n","    kp, desc = feature_extractor.detectAndCompute(img, None)\n","    if desc is not None:\n","      all_desc.append((desc, labels[i]))\n","      desc = desc[:num_desc]\n","      descriptors = np.concatenate((descriptors, desc))\n","  return descriptors, np.array(all_desc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwaO9B80A9ZB"},"outputs":[],"source":["def get_descriptors(all_descriptors, max_desc_num):\n","  \"\"\"\n","  This function will use the exported descriptors to extract a list of all\n","  desciptors from a dataset such that each image only provides a max number of\n","  descriptors (max_desc_num)\n","  \"\"\"\n","  descriptors = list(all_descriptors[0][0][:max_desc_num])\n","  for i, d in enumerate(all_descriptors[1:]):\n","    if i % 300 == 0:\n","      print(i)\n","    descriptors = descriptors + list(d[0][:max_desc_num])\n","  return np.array(descriptors)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjPMns_HPwSQ"},"outputs":[],"source":["# # DANGER: only run if the files don't already exist - takes a long time to run (30 min)\n","\n","# descriptors_0000, all_descriptors_0000 = extract_all_descriptors(images_0000, labels_0000, NUM_DESC)\n","# np.save(os.path.join(path, 'descriptors_0000'), descriptors_0000)\n","# np.save(os.path.join(path, 'all_descriptors_0000'), all_descriptors_0000)\n","# descriptors_0001, all_descriptors_0001 = extract_all_descriptors(images_0001, labels_0001, NUM_DESC)\n","# np.save(os.path.join(path, 'descriptors_0001'), descriptors_0001)\n","# np.save(os.path.join(path, 'all_descriptors_0001'), all_descriptors_0001)\n","# descriptors_0002, all_descriptors_0002 = extract_all_descriptors(images_0002, labels_0002, NUM_DESC)\n","# np.save(os.path.join(path, 'descriptors_0002'), descriptors_0002)\n","# np.save(os.path.join(path, 'all_descriptors_0002'), all_descriptors_0002)"]},{"cell_type":"code","source":["# DANGER: only run if the files don't already exist - takes a long time to run (30 min)\n","\n","extra_descriptors_0000, extra_all_descriptors_0000 = extract_all_descriptors(extra_images_0000, extra_labels_0000, NUM_DESC)\n","np.save(os.path.join(path, 'extra_descriptors_0000'), extra_descriptors_0000)\n","np.save(os.path.join(path, 'extra_all_descriptors_0000'), extra_all_descriptors_0000)\n","extra_descriptors_0001, extra_all_descriptors_0001 = extract_all_descriptors(extra_images_0001, extra_labels_0001, NUM_DESC)\n","np.save(os.path.join(path, 'extra_descriptors_0001'), extra_descriptors_0001)\n","np.save(os.path.join(path, 'extra_all_descriptors_0001'), extra_all_descriptors_0001)\n","extra_descriptors_0002, extra_all_descriptors_0002 = extract_all_descriptors(extra_images_0002, extra_labels_0002, NUM_DESC)\n","np.save(os.path.join(path, 'extra_descriptors_0002'), extra_descriptors_0002)\n","np.save(os.path.join(path, 'extra_all_descriptors_0002'), extra_all_descriptors_0002)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTx8BumpGyDH","executionInfo":{"status":"ok","timestamp":1648917921786,"user_tz":240,"elapsed":830844,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"}},"outputId":"694ac587-bcdf-4671-f0ec-cc68be26c958"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpdSztQRtgKy"},"outputs":[],"source":["descriptors_0000 = np.load(os.path.join(path, 'descriptors_0000.npy'), allow_pickle=True)\n","descriptors_0001 = np.load(os.path.join(path, 'descriptors_0001.npy'), allow_pickle=True)\n","descriptors_0002 = np.load(os.path.join(path, 'descriptors_0002.npy'), allow_pickle=True)\n","\n","all_descriptors_0000 = np.load(os.path.join(path, 'all_descriptors_0000.npy'), allow_pickle=True)\n","all_descriptors_0001 = np.load(os.path.join(path, 'all_descriptors_0001.npy'), allow_pickle=True)\n","all_descriptors_0002 = np.load(os.path.join(path, 'all_descriptors_0002.npy'), allow_pickle=True)"]},{"cell_type":"markdown","metadata":{"id":"Hu13JIGAx6FH"},"source":["### 2 - KMeans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rHyhv8CyQVb"},"outputs":[],"source":["NUM_CLUSTERS = 250"]},{"cell_type":"code","source":["# descriptors_0000 = get_descriptors(all_descriptors_0000, 100)\n","# mask = all_descriptors_0001[:, 1] == 0.\n","# pos_desc = all_descriptors_0001[~mask]\n","# neg_desc = all_descriptors_0001[mask]\n","# idx = np.random.randint(neg_desc.shape[0], size=2888)\n","# neg_desc = neg_desc[idx, :]\n","# print(pos_desc.shape, neg_desc.shape)\n","# all_desc = np.concatenate((pos_desc, neg_desc))\n","# print(all_desc.shape)\n","# descriptors_0001 = get_descriptors(all_desc, 100)"],"metadata":{"id":"GpSxahOWlreN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_descriptors_0_1 = np.concatenate((descriptors_0000, descriptors_0001, extra_descriptors_0000, extra_descriptors_0001))"],"metadata":{"id":"B1AnjZeBouMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-amt4R56x6av"},"outputs":[],"source":["# Training KMeans takes about 10 min fro 10 clusters\n","\n","\n","# Validation kmeans\n","kmeans_00_01 = KMeans(n_clusters=NUM_CLUSTERS, random_state=0).fit(train_descriptors_0_1)\n","# kmeans_00_02 = KMeans(n_clusters=NUM_CLUSTERS, random_state=0).fit(np.concatenate((descriptors_0000, descriptors_0002)))\n","# kmeans_01_02 = KMeans(n_clusters=NUM_CLUSTERS, random_state=0).fit(np.concatenate((descriptors_0001, descriptors_0002)))\n","\n","# # Will be used as final model - test on 03\n","# kmeans_00_01_02 = KMeans(n_clusters=NUM_CLUSTERS, random_state=0).fit(np.concatenate((descriptors_0000, descriptors_0001, descriptors_0002)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6sPEFAOECRl"},"outputs":[],"source":["with open(os.path.join(path, f\"kmeans_00_01_75_{NUM_CLUSTERS}.pkl\"), \"wb\") as f:\n","    pickle.dump(kmeans_00_01, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvMUaYO8EXGz"},"outputs":[],"source":["with open(os.path.join(path, f\"kmeans_00_01_{NUM_CLUSTERS}.pkl\"), \"rb\") as f:\n","    kmeans_00_01 = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"QWFrGQeGTUcs"},"source":["### 3 - Validation Tests"]},{"cell_type":"markdown","metadata":{"id":"pRFM1AIccWqh"},"source":["#### Helper Methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jH7KqhbpfWd9"},"outputs":[],"source":["'''\n","Create bin for each sample based on what clusters its sift descriptors belong to.\n","Normalize all bins. These will be used as training samples.\n","'''\n","\n","def desc_to_norm_cluster_bin(kmeans, desc, num_clusters):\n","  \"\"\"\n","  This function will take all the descriptors in desc and find the \n","  closest clusters for each one using the trained kmeans model passed.\n","  It will produce a bincount of how many times each cluster is seen and normalize\n","  this.\n","  So no matter how many descriptors are passed (we should keep this constant),\n","  we will output a normalized vector of length NUM_CLUSTERS.\n","  \"\"\"\n","  clusters = kmeans.predict(list(desc))\n","  bin = np.bincount(clusters)\n","  if len(bin) < num_clusters:\n","    bin = np.pad(bin, (0, num_clusters - len(bin)), 'constant')\n","  norm_bin = bin / np.sum(bin)\n","  return norm_bin\n","\n","\n","def descriptors_to_bins(kmeans, descriptors, num_clusters):\n","  \"\"\"\n","  This function takes in a kmeans model as well as a list containing tuples (descriptors).\n","  The tuple has 2 elements, the first is the descriptors for an image, and the second\n","  is a label.\n","\n","  It will produce a dataset where the descriptors of each image are transformed\n","  into normalized bin vector of size NUM_CLUSTERS\n","  \"\"\"\n","\n","  X = np.empty((len(descriptors), num_clusters))\n","  y = np.empty(len(descriptors), dtype=float)\n","  for i, (desc, label) in enumerate(descriptors):\n","    desc = desc.astype(float)\n","    norm_bin = desc_to_norm_cluster_bin(kmeans, desc, num_clusters)\n","\n","    X[i] = norm_bin\n","    y[i] = label\n","  return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faY8X7b7Yb9o"},"outputs":[],"source":["def validation(images, labels, kmeans, model, num_clusters):\n","  \"\"\"\n","  Give a set of images, labels a kmeans model as well as a general model\n","  that accept vectors of dimension ${num_clusters}, this method will use the \n","  model to make predictions on all the images and return statistics on its\n","  performance.\n","  \"\"\"\n","\n","  predictions = np.empty(len(images), dtype=float)\n","  count = 0\n","  for i, img in enumerate(images):\n","    feature_extractor = cv2.SIFT_create()\n","    kp, desc = feature_extractor.detectAndCompute(img, None)\n","    if desc is None:\n","      predictions[i] = 0.\n","      count+=1\n","      continue\n","    desc = desc.astype(float)\n","    norm_bin = desc_to_norm_cluster_bin(kmeans, desc, num_clusters)\n","    predictions[i] = model.predict([norm_bin])\n","\n","  acc = np.sum(predictions == labels) / len(labels)\n","  precision, recall, _, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n","  print(count)\n","  return acc, precision, recall, predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJQ1UmuSacyH"},"outputs":[],"source":["def train_and_predict(test_images, labels, train_descriptors, num_clusters, model, kmeans):\n","  X,y = descriptors_to_bins(kmeans, train_descriptors, NUM_CLUSTERS)\n","  model.fit(X,y)\n","  acc, precision, recall, predictions = validation(test_images, labels, kmeans, model, num_clusters)\n","  return model, acc, precision, recall, predictions"]},{"cell_type":"markdown","metadata":{"id":"PBrcVhr1cZrG"},"source":["#### Validation Tests"]},{"cell_type":"markdown","metadata":{"id":"GhsBvwWmcgjB"},"source":["##### SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248544,"status":"ok","timestamp":1648930316163,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"e8iGouNUbiu3","outputId":"e32d998b-5df8-402d-b67b-d84a3b1f9804"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=30000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["104\n"]}],"source":["# Validation Test 1: Trained on 0000 and 0001 - Test on 0002\n","\n","clf_01 = make_pipeline(MinMaxScaler(), SVC(kernel='rbf', degree=6, random_state=0, tol=1e-5, max_iter=30000, C=1))\n","model_1, acc1, p1, r1, pred1 = train_and_predict(\n","                  images_0002, \n","                  labels_0002, \n","                  np.concatenate((all_descriptors_0000, all_descriptors_0001)), \n","                  NUM_CLUSTERS, \n","                  clf_01, \n","                  kmeans_00_01\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1648674105281,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"Yq4vK5IqjFWE","outputId":"9016143d-10c0-433d-95c6-b47bb987e0be"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.3489749430523918 0.20500595947556616 0.7835990888382688\n","tn: 844, fp: 2668, fn: 190, tp: 688\n"]}],"source":[" # C = 100000, kmeans = 200 clusters\n","print(acc1, p1, r1)\n","tn, fp, fn, tp = confusion_matrix(labels_0002, pred1).ravel()\n","print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1648928394418,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"FNg8ob6qCjbK","outputId":"1ca073d6-11b8-4ee2-eccf-f3336b5f1851"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.6266514806378132 0.24445936870382806 0.4145785876993166\n","tn: 2387, fp: 1125, fn: 514, tp: 364\n"]}],"source":["# C = 10000, kmeans = 500 clusters\n","print(acc1, p1, r1)\n","tn, fp, fn, tp = confusion_matrix(labels_0002, pred1).ravel()\n","print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52436,"status":"ok","timestamp":1648930533445,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"YXzrLI_XCUl5","outputId":"c790e9bd-24e6-47aa-f315-d24c8756c721"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=30000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["104\n"]}],"source":["# Validation Test 1: Trained on 0000 and 0001 - Test on 0002\n","\n","clf_01 = make_pipeline(StandardScaler(), SVC(kernel='linear', degree=6, random_state=0, tol=1e-5, max_iter=30000, C=10000))\n","model_1, acc1, p1, r1, pred1 = train_and_predict(\n","                  images_0002, \n","                  labels_0002, \n","                  np.concatenate((all_descriptors_0000, all_descriptors_0001)), \n","                  NUM_CLUSTERS, \n","                  clf_01,  \n","                  kmeans_00_01\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1648930533446,"user":{"displayName":"ECSE 415","userId":"01135113846004908202"},"user_tz":240},"id":"beSlrkmBjIOx","outputId":"0ae2cfde-8320-4bc0-f621-1e3f75b99f9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.3548974943052392 0.20730976632714201 0.7881548974943052\n","tn: 866, fp: 2646, fn: 186, tp: 692\n"]}],"source":["print(acc1, p1, r1)\n","tn, fp, fn, tp = confusion_matrix(labels_0002, pred1).ravel()\n","print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoEizHiGbrA1"},"outputs":[],"source":["# Validation Test 2: Trained on 0000 and 0002 - Test on 0001\n","\n","clf_02 = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=0, tol=1e-5, max_iter=30000))\n","model_2, acc2, p2, r2, pred2 = train_and_predict(\n","                  images_0001, \n","                  labels_0001, \n","                  np.concatenate((all_descriptors_0000, all_descriptors_0002)), \n","                  NUM_CLUSTERS, \n","                  clf_02, \n","                  kmeans_00_02\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJAmrseVdDMN"},"outputs":[],"source":["# Validation Test 3: Trained on 0001 and 0002 - Test on 0000\n","\n","clf_12 = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=0, tol=1e-5, max_iter=30000))\n","model_3, acc3, p3, r3, pred3 = train_and_predict(\n","                  images_0000, \n","                  labels_0000, \n","                  np.concatenate((all_descriptors_0001, all_descriptors_0002)), \n","                  NUM_CLUSTERS, \n","                  clf_12, \n","                  kmeans_01_02\n","                  )"]},{"cell_type":"markdown","metadata":{"id":"lJw5UBZccjXS"},"source":["##### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vF5gXzYydVkv"},"outputs":[],"source":["# Validation Test 1: Trained on 0000 and 0001 - Test on 0002\n","\n","clf_lr_01 = LogisticRegression(random_state=0)\n","model_1, acc1, p1, r1, pred1 = train_and_predict(\n","                  images_0002, \n","                  labels_0002, \n","                  np.concatenate((all_descriptors_0000, all_descriptors_0001)), \n","                  NUM_CLUSTERS, \n","                  clf_01, \n","                  kmeans_00_01\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh3z2uJqdifK"},"outputs":[],"source":["# Validation Test 2: Trained on 0000 and 0002 - Test on 0001\n","\n","clf_02 = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, max_iter=30000))\n","model_2, acc2, p2, r2, pred2 = train_and_predict(\n","                  images_0001, \n","                  labels_0001, \n","                  np.concatenate((all_descriptors_0000, all_descriptors_0002)), \n","                  NUM_CLUSTERS, \n","                  clf_02, \n","                  kmeans_00_02\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ1Fctjndj06"},"outputs":[],"source":["# Validation Test 3: Trained on 0001 and 0002 - Test on 0000\n","\n","clf_12 = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, max_iter=30000))\n","model_3, acc3, p3, r3, pred3 = train_and_predict(\n","                  images_0000, \n","                  labels_0000, \n","                  np.concatenate((all_descriptors_0001, all_descriptors_0002)), \n","                  NUM_CLUSTERS, \n","                  clf_12, \n","                  kmeans_01_02\n","                  )"]}],"metadata":{"colab":{"collapsed_sections":["cDhtdEwKQc5H","AgtQ-Q4URXKK","pz04GGSUQ-TD","Hu13JIGAx6FH","pRFM1AIccWqh","GhsBvwWmcgjB","lJw5UBZccjXS"],"name":"Classifier_SIFT.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}